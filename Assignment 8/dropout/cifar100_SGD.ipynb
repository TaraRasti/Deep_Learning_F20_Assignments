{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar100-SGD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNddKsoNkml9Xrw+Q/LElt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaraRasti/Deep_Learning_F20_Assignments/blob/master/Assignment%208/dropout/cifar100_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewE4zO_bsTJI"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZWSRMxjsZHU"
      },
      "source": [
        "train=True\n",
        "num_classes = 100\n",
        "weight_decay = 0.0005\n",
        "x_shape = [32,32,3]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjKKB16BseOk"
      },
      "source": [
        "model = Sequential()\n",
        "weight_decay = weight_decay\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                 input_shape=x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55_ntF5Rsh-0"
      },
      "source": [
        "def normalize(X_train,X_test):\n",
        "    mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "    print(mean)\n",
        "    print(std)\n",
        "    X_train = (X_train-mean)/(std+1e-7)\n",
        "    X_test = (X_test-mean)/(std+1e-7)\n",
        "    return X_train, X_test\n",
        "\n",
        "def normalize_production(x):\n",
        "    mean = 121.936\n",
        "    std = 68.389\n",
        "    return (x-mean)/(std+1e-7)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98psLfS8skix"
      },
      "source": [
        "def predict(x,normalize=True,batch_size=50):\n",
        "    if normalize:\n",
        "        x = normalize_production(x)\n",
        "    return model.predict(x,batch_size)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWkHfijmsnjv",
        "outputId": "db2bff87-0618-44ac-daca-7fb75c2326bc"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train, x_test = normalize(x_train, x_test)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n",
            "121.93584\n",
            "68.38902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh17Y_Xbspts",
        "outputId": "b0f85099-393f-41ce-947c-319365fe8bd5"
      },
      "source": [
        "batch_size = 128\n",
        "maxepoches = 200\n",
        "learning_rate = 0.1\n",
        "lr_decay = 1e-6\n",
        "lr_drop = 20\n",
        "\n",
        "def lr_scheduler(epoch):\n",
        "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  \n",
        "    samplewise_center=False,  \n",
        "    featurewise_std_normalization=False,  \n",
        "    samplewise_std_normalization=False,  \n",
        "    zca_whitening=False,  \n",
        "    rotation_range=15,  \n",
        "    width_shift_range=0.1,  \n",
        "    height_shift_range=0.1,  \n",
        "    horizontal_flip=True,  \n",
        "    vertical_flip=False)  \n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "\n",
        "sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "hist = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                 batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                    epochs=maxepoches,\n",
        "                    validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=2)\n",
        "model.save_weights('cifar100')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-683aea1407ce>:37: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/200\n",
            "390/390 - 61s - loss: 17.8915 - accuracy: 0.0240 - val_loss: 13.6556 - val_accuracy: 0.0173\n",
            "Epoch 2/200\n",
            "390/390 - 59s - loss: 10.4998 - accuracy: 0.0463 - val_loss: 8.7983 - val_accuracy: 0.0201\n",
            "Epoch 3/200\n",
            "390/390 - 59s - loss: 7.0244 - accuracy: 0.0578 - val_loss: 6.6646 - val_accuracy: 0.0292\n",
            "Epoch 4/200\n",
            "390/390 - 59s - loss: 5.4353 - accuracy: 0.0773 - val_loss: 5.3462 - val_accuracy: 0.0466\n",
            "Epoch 5/200\n",
            "390/390 - 59s - loss: 4.6738 - accuracy: 0.0917 - val_loss: 4.5668 - val_accuracy: 0.0826\n",
            "Epoch 6/200\n",
            "390/390 - 59s - loss: 4.2799 - accuracy: 0.1110 - val_loss: 4.1799 - val_accuracy: 0.1243\n",
            "Epoch 7/200\n",
            "390/390 - 59s - loss: 4.0829 - accuracy: 0.1275 - val_loss: 4.2777 - val_accuracy: 0.1287\n",
            "Epoch 8/200\n",
            "390/390 - 59s - loss: 3.9622 - accuracy: 0.1589 - val_loss: 4.0214 - val_accuracy: 0.1656\n",
            "Epoch 9/200\n",
            "390/390 - 59s - loss: 3.8620 - accuracy: 0.1885 - val_loss: 3.6767 - val_accuracy: 0.2260\n",
            "Epoch 10/200\n",
            "390/390 - 59s - loss: 3.7738 - accuracy: 0.2154 - val_loss: 3.5712 - val_accuracy: 0.2583\n",
            "Epoch 11/200\n",
            "390/390 - 59s - loss: 3.7271 - accuracy: 0.2375 - val_loss: 3.6588 - val_accuracy: 0.2456\n",
            "Epoch 12/200\n",
            "390/390 - 59s - loss: 3.7079 - accuracy: 0.2553 - val_loss: 3.8166 - val_accuracy: 0.2581\n",
            "Epoch 13/200\n",
            "390/390 - 59s - loss: 3.6913 - accuracy: 0.2724 - val_loss: 3.7021 - val_accuracy: 0.2778\n",
            "Epoch 14/200\n",
            "390/390 - 59s - loss: 3.6999 - accuracy: 0.2816 - val_loss: 3.7553 - val_accuracy: 0.2891\n",
            "Epoch 15/200\n",
            "390/390 - 59s - loss: 3.7021 - accuracy: 0.2945 - val_loss: 3.5998 - val_accuracy: 0.3186\n",
            "Epoch 16/200\n",
            "390/390 - 59s - loss: 3.7192 - accuracy: 0.3015 - val_loss: 3.5866 - val_accuracy: 0.3414\n",
            "Epoch 17/200\n",
            "390/390 - 59s - loss: 3.7369 - accuracy: 0.3086 - val_loss: 3.6056 - val_accuracy: 0.3487\n",
            "Epoch 18/200\n",
            "390/390 - 59s - loss: 3.7413 - accuracy: 0.3209 - val_loss: 3.8986 - val_accuracy: 0.3283\n",
            "Epoch 19/200\n",
            "390/390 - 59s - loss: 3.7451 - accuracy: 0.3265 - val_loss: 3.6335 - val_accuracy: 0.3579\n",
            "Epoch 20/200\n",
            "390/390 - 59s - loss: 3.7737 - accuracy: 0.3310 - val_loss: 3.7012 - val_accuracy: 0.3507\n",
            "Epoch 21/200\n",
            "390/390 - 59s - loss: 3.4467 - accuracy: 0.3894 - val_loss: 3.1808 - val_accuracy: 0.4279\n",
            "Epoch 22/200\n",
            "390/390 - 59s - loss: 3.2824 - accuracy: 0.4051 - val_loss: 3.2229 - val_accuracy: 0.4166\n",
            "Epoch 23/200\n",
            "390/390 - 59s - loss: 3.2553 - accuracy: 0.4086 - val_loss: 3.1148 - val_accuracy: 0.4385\n",
            "Epoch 24/200\n",
            "390/390 - 59s - loss: 3.2243 - accuracy: 0.4106 - val_loss: 3.1310 - val_accuracy: 0.4331\n",
            "Epoch 25/200\n",
            "390/390 - 59s - loss: 3.2345 - accuracy: 0.4177 - val_loss: 3.1826 - val_accuracy: 0.4299\n",
            "Epoch 26/200\n",
            "390/390 - 59s - loss: 3.2347 - accuracy: 0.4183 - val_loss: 3.2889 - val_accuracy: 0.4259\n",
            "Epoch 27/200\n",
            "390/390 - 59s - loss: 3.2479 - accuracy: 0.4250 - val_loss: 3.2065 - val_accuracy: 0.4450\n",
            "Epoch 28/200\n",
            "390/390 - 59s - loss: 3.2549 - accuracy: 0.4319 - val_loss: 3.1335 - val_accuracy: 0.4676\n",
            "Epoch 29/200\n",
            "390/390 - 59s - loss: 3.2681 - accuracy: 0.4309 - val_loss: 3.1339 - val_accuracy: 0.4635\n",
            "Epoch 30/200\n",
            "390/390 - 59s - loss: 3.2779 - accuracy: 0.4369 - val_loss: 3.4897 - val_accuracy: 0.4110\n",
            "Epoch 31/200\n",
            "390/390 - 59s - loss: 3.2778 - accuracy: 0.4386 - val_loss: 3.3975 - val_accuracy: 0.4286\n",
            "Epoch 32/200\n",
            "390/390 - 59s - loss: 3.2953 - accuracy: 0.4405 - val_loss: 3.5114 - val_accuracy: 0.4144\n",
            "Epoch 33/200\n",
            "390/390 - 59s - loss: 3.3057 - accuracy: 0.4423 - val_loss: 3.3865 - val_accuracy: 0.4434\n",
            "Epoch 34/200\n",
            "390/390 - 59s - loss: 3.3047 - accuracy: 0.4468 - val_loss: 3.1884 - val_accuracy: 0.4736\n",
            "Epoch 35/200\n",
            "390/390 - 59s - loss: 3.3020 - accuracy: 0.4492 - val_loss: 3.3713 - val_accuracy: 0.4504\n",
            "Epoch 36/200\n",
            "390/390 - 59s - loss: 3.3103 - accuracy: 0.4523 - val_loss: 3.1456 - val_accuracy: 0.4865\n",
            "Epoch 37/200\n",
            "390/390 - 59s - loss: 3.3147 - accuracy: 0.4573 - val_loss: 3.5125 - val_accuracy: 0.4394\n",
            "Epoch 38/200\n",
            "390/390 - 59s - loss: 3.3090 - accuracy: 0.4601 - val_loss: 3.2769 - val_accuracy: 0.4747\n",
            "Epoch 39/200\n",
            "390/390 - 59s - loss: 3.3163 - accuracy: 0.4607 - val_loss: 3.3711 - val_accuracy: 0.4614\n",
            "Epoch 40/200\n",
            "390/390 - 59s - loss: 3.3344 - accuracy: 0.4600 - val_loss: 3.2554 - val_accuracy: 0.4802\n",
            "Epoch 41/200\n",
            "390/390 - 59s - loss: 3.0493 - accuracy: 0.5130 - val_loss: 2.9892 - val_accuracy: 0.5272\n",
            "Epoch 42/200\n",
            "390/390 - 59s - loss: 2.9029 - accuracy: 0.5317 - val_loss: 2.9098 - val_accuracy: 0.5295\n",
            "Epoch 43/200\n",
            "390/390 - 59s - loss: 2.8347 - accuracy: 0.5322 - val_loss: 3.0474 - val_accuracy: 0.5039\n",
            "Epoch 44/200\n",
            "390/390 - 59s - loss: 2.7967 - accuracy: 0.5365 - val_loss: 3.1100 - val_accuracy: 0.4858\n",
            "Epoch 45/200\n",
            "390/390 - 59s - loss: 2.7936 - accuracy: 0.5338 - val_loss: 2.7175 - val_accuracy: 0.5500\n",
            "Epoch 46/200\n",
            "390/390 - 59s - loss: 2.7669 - accuracy: 0.5378 - val_loss: 2.8276 - val_accuracy: 0.5348\n",
            "Epoch 47/200\n",
            "390/390 - 59s - loss: 2.7717 - accuracy: 0.5395 - val_loss: 2.8393 - val_accuracy: 0.5312\n",
            "Epoch 48/200\n",
            "390/390 - 59s - loss: 2.7692 - accuracy: 0.5385 - val_loss: 2.7598 - val_accuracy: 0.5466\n",
            "Epoch 49/200\n",
            "390/390 - 59s - loss: 2.7675 - accuracy: 0.5381 - val_loss: 2.8713 - val_accuracy: 0.5326\n",
            "Epoch 50/200\n",
            "390/390 - 59s - loss: 2.7810 - accuracy: 0.5421 - val_loss: 2.7918 - val_accuracy: 0.5416\n",
            "Epoch 51/200\n",
            "390/390 - 59s - loss: 2.7734 - accuracy: 0.5464 - val_loss: 2.7877 - val_accuracy: 0.5554\n",
            "Epoch 52/200\n",
            "390/390 - 59s - loss: 2.7827 - accuracy: 0.5447 - val_loss: 2.9177 - val_accuracy: 0.5302\n",
            "Epoch 53/200\n",
            "390/390 - 59s - loss: 2.7990 - accuracy: 0.5449 - val_loss: 2.7572 - val_accuracy: 0.5651\n",
            "Epoch 54/200\n",
            "390/390 - 59s - loss: 2.7863 - accuracy: 0.5486 - val_loss: 2.8388 - val_accuracy: 0.5434\n",
            "Epoch 55/200\n",
            "390/390 - 59s - loss: 2.7924 - accuracy: 0.5525 - val_loss: 2.9248 - val_accuracy: 0.5253\n",
            "Epoch 56/200\n",
            "390/390 - 59s - loss: 2.7841 - accuracy: 0.5530 - val_loss: 2.7568 - val_accuracy: 0.5651\n",
            "Epoch 57/200\n",
            "390/390 - 59s - loss: 2.7846 - accuracy: 0.5558 - val_loss: 2.9812 - val_accuracy: 0.5313\n",
            "Epoch 58/200\n",
            "390/390 - 59s - loss: 2.7911 - accuracy: 0.5568 - val_loss: 2.8207 - val_accuracy: 0.5623\n",
            "Epoch 59/200\n",
            "390/390 - 59s - loss: 2.8046 - accuracy: 0.5570 - val_loss: 2.8557 - val_accuracy: 0.5561\n",
            "Epoch 60/200\n",
            "390/390 - 59s - loss: 2.7970 - accuracy: 0.5574 - val_loss: 2.8948 - val_accuracy: 0.5568\n",
            "Epoch 61/200\n",
            "390/390 - 59s - loss: 2.6010 - accuracy: 0.6028 - val_loss: 2.6752 - val_accuracy: 0.5903\n",
            "Epoch 62/200\n",
            "390/390 - 59s - loss: 2.4810 - accuracy: 0.6215 - val_loss: 2.5253 - val_accuracy: 0.6092\n",
            "Epoch 63/200\n",
            "390/390 - 59s - loss: 2.4288 - accuracy: 0.6248 - val_loss: 2.5969 - val_accuracy: 0.5959\n",
            "Epoch 64/200\n",
            "390/390 - 59s - loss: 2.3934 - accuracy: 0.6252 - val_loss: 2.4635 - val_accuracy: 0.6122\n",
            "Epoch 65/200\n",
            "390/390 - 59s - loss: 2.3609 - accuracy: 0.6269 - val_loss: 2.4821 - val_accuracy: 0.6084\n",
            "Epoch 66/200\n",
            "390/390 - 59s - loss: 2.3432 - accuracy: 0.6273 - val_loss: 2.4332 - val_accuracy: 0.6131\n",
            "Epoch 67/200\n",
            "390/390 - 59s - loss: 2.3426 - accuracy: 0.6261 - val_loss: 2.4080 - val_accuracy: 0.6217\n",
            "Epoch 68/200\n",
            "390/390 - 59s - loss: 2.3090 - accuracy: 0.6329 - val_loss: 2.4137 - val_accuracy: 0.6168\n",
            "Epoch 69/200\n",
            "390/390 - 59s - loss: 2.3151 - accuracy: 0.6305 - val_loss: 2.5144 - val_accuracy: 0.5989\n",
            "Epoch 70/200\n",
            "390/390 - 59s - loss: 2.3086 - accuracy: 0.6332 - val_loss: 2.4937 - val_accuracy: 0.5946\n",
            "Epoch 71/200\n",
            "390/390 - 58s - loss: 2.2952 - accuracy: 0.6321 - val_loss: 2.4893 - val_accuracy: 0.6059\n",
            "Epoch 72/200\n",
            "390/390 - 58s - loss: 2.3081 - accuracy: 0.6310 - val_loss: 2.4633 - val_accuracy: 0.6032\n",
            "Epoch 73/200\n",
            "390/390 - 59s - loss: 2.3006 - accuracy: 0.6331 - val_loss: 2.4467 - val_accuracy: 0.6123\n",
            "Epoch 74/200\n",
            "390/390 - 59s - loss: 2.3028 - accuracy: 0.6352 - val_loss: 2.6386 - val_accuracy: 0.5861\n",
            "Epoch 75/200\n",
            "390/390 - 59s - loss: 2.3138 - accuracy: 0.6334 - val_loss: 2.5650 - val_accuracy: 0.5963\n",
            "Epoch 76/200\n",
            "390/390 - 59s - loss: 2.2882 - accuracy: 0.6390 - val_loss: 2.4871 - val_accuracy: 0.6075\n",
            "Epoch 77/200\n",
            "390/390 - 59s - loss: 2.3015 - accuracy: 0.6374 - val_loss: 2.4727 - val_accuracy: 0.6072\n",
            "Epoch 78/200\n",
            "390/390 - 59s - loss: 2.3055 - accuracy: 0.6347 - val_loss: 2.4953 - val_accuracy: 0.6081\n",
            "Epoch 79/200\n",
            "390/390 - 59s - loss: 2.2989 - accuracy: 0.6396 - val_loss: 2.4709 - val_accuracy: 0.6143\n",
            "Epoch 80/200\n",
            "390/390 - 59s - loss: 2.3077 - accuracy: 0.6400 - val_loss: 2.6133 - val_accuracy: 0.5919\n",
            "Epoch 81/200\n",
            "390/390 - 59s - loss: 2.1522 - accuracy: 0.6756 - val_loss: 2.3805 - val_accuracy: 0.6374\n",
            "Epoch 82/200\n",
            "390/390 - 59s - loss: 2.0702 - accuracy: 0.6902 - val_loss: 2.2639 - val_accuracy: 0.6521\n",
            "Epoch 83/200\n",
            "390/390 - 59s - loss: 2.0169 - accuracy: 0.6971 - val_loss: 2.3333 - val_accuracy: 0.6391\n",
            "Epoch 84/200\n",
            "390/390 - 59s - loss: 1.9992 - accuracy: 0.7004 - val_loss: 2.3005 - val_accuracy: 0.6455\n",
            "Epoch 85/200\n",
            "390/390 - 59s - loss: 1.9695 - accuracy: 0.7048 - val_loss: 2.2872 - val_accuracy: 0.6432\n",
            "Epoch 86/200\n",
            "390/390 - 59s - loss: 1.9610 - accuracy: 0.7011 - val_loss: 2.2901 - val_accuracy: 0.6418\n",
            "Epoch 87/200\n",
            "390/390 - 59s - loss: 1.9205 - accuracy: 0.7098 - val_loss: 2.2504 - val_accuracy: 0.6510\n",
            "Epoch 88/200\n",
            "390/390 - 59s - loss: 1.9132 - accuracy: 0.7102 - val_loss: 2.2751 - val_accuracy: 0.6477\n",
            "Epoch 89/200\n",
            "390/390 - 59s - loss: 1.9113 - accuracy: 0.7051 - val_loss: 2.2821 - val_accuracy: 0.6443\n",
            "Epoch 90/200\n",
            "390/390 - 59s - loss: 1.8994 - accuracy: 0.7081 - val_loss: 2.1798 - val_accuracy: 0.6549\n",
            "Epoch 91/200\n",
            "390/390 - 59s - loss: 1.9029 - accuracy: 0.7065 - val_loss: 2.2480 - val_accuracy: 0.6485\n",
            "Epoch 92/200\n",
            "390/390 - 59s - loss: 1.8845 - accuracy: 0.7092 - val_loss: 2.1939 - val_accuracy: 0.6573\n",
            "Epoch 93/200\n",
            "390/390 - 59s - loss: 1.8896 - accuracy: 0.7085 - val_loss: 2.3120 - val_accuracy: 0.6385\n",
            "Epoch 94/200\n",
            "390/390 - 59s - loss: 1.8904 - accuracy: 0.7065 - val_loss: 2.2564 - val_accuracy: 0.6458\n",
            "Epoch 95/200\n",
            "390/390 - 59s - loss: 1.8828 - accuracy: 0.7081 - val_loss: 2.2264 - val_accuracy: 0.6460\n",
            "Epoch 96/200\n",
            "390/390 - 59s - loss: 1.8730 - accuracy: 0.7123 - val_loss: 2.2426 - val_accuracy: 0.6450\n",
            "Epoch 97/200\n",
            "390/390 - 59s - loss: 1.8685 - accuracy: 0.7128 - val_loss: 2.2646 - val_accuracy: 0.6426\n",
            "Epoch 98/200\n",
            "390/390 - 59s - loss: 1.8645 - accuracy: 0.7127 - val_loss: 2.1997 - val_accuracy: 0.6584\n",
            "Epoch 99/200\n",
            "390/390 - 59s - loss: 1.8688 - accuracy: 0.7114 - val_loss: 2.2345 - val_accuracy: 0.6494\n",
            "Epoch 100/200\n",
            "390/390 - 59s - loss: 1.8701 - accuracy: 0.7130 - val_loss: 2.2701 - val_accuracy: 0.6458\n",
            "Epoch 101/200\n",
            "390/390 - 59s - loss: 1.7479 - accuracy: 0.7418 - val_loss: 2.1894 - val_accuracy: 0.6640\n",
            "Epoch 102/200\n",
            "390/390 - 59s - loss: 1.6827 - accuracy: 0.7575 - val_loss: 2.1934 - val_accuracy: 0.6597\n",
            "Epoch 103/200\n",
            "390/390 - 59s - loss: 1.6681 - accuracy: 0.7564 - val_loss: 2.0866 - val_accuracy: 0.6834\n",
            "Epoch 104/200\n",
            "390/390 - 59s - loss: 1.6314 - accuracy: 0.7653 - val_loss: 2.2124 - val_accuracy: 0.6611\n",
            "Epoch 105/200\n",
            "390/390 - 59s - loss: 1.6149 - accuracy: 0.7665 - val_loss: 2.1723 - val_accuracy: 0.6680\n",
            "Epoch 106/200\n",
            "390/390 - 59s - loss: 1.6082 - accuracy: 0.7658 - val_loss: 2.1382 - val_accuracy: 0.6714\n",
            "Epoch 107/200\n",
            "390/390 - 59s - loss: 1.5806 - accuracy: 0.7717 - val_loss: 2.1664 - val_accuracy: 0.6666\n",
            "Epoch 108/200\n",
            "390/390 - 59s - loss: 1.5686 - accuracy: 0.7744 - val_loss: 2.1159 - val_accuracy: 0.6742\n",
            "Epoch 109/200\n",
            "390/390 - 59s - loss: 1.5720 - accuracy: 0.7706 - val_loss: 2.1775 - val_accuracy: 0.6666\n",
            "Epoch 110/200\n",
            "390/390 - 59s - loss: 1.5615 - accuracy: 0.7724 - val_loss: 2.0825 - val_accuracy: 0.6785\n",
            "Epoch 111/200\n",
            "390/390 - 59s - loss: 1.5399 - accuracy: 0.7775 - val_loss: 2.1963 - val_accuracy: 0.6648\n",
            "Epoch 112/200\n",
            "390/390 - 59s - loss: 1.5346 - accuracy: 0.7776 - val_loss: 2.0987 - val_accuracy: 0.6775\n",
            "Epoch 113/200\n",
            "390/390 - 59s - loss: 1.5373 - accuracy: 0.7751 - val_loss: 2.1293 - val_accuracy: 0.6727\n",
            "Epoch 114/200\n",
            "390/390 - 59s - loss: 1.5212 - accuracy: 0.7784 - val_loss: 2.1079 - val_accuracy: 0.6752\n",
            "Epoch 115/200\n",
            "390/390 - 59s - loss: 1.5052 - accuracy: 0.7833 - val_loss: 2.1398 - val_accuracy: 0.6711\n",
            "Epoch 116/200\n",
            "390/390 - 59s - loss: 1.5128 - accuracy: 0.7809 - val_loss: 2.1629 - val_accuracy: 0.6598\n",
            "Epoch 117/200\n",
            "390/390 - 59s - loss: 1.5090 - accuracy: 0.7811 - val_loss: 2.0989 - val_accuracy: 0.6760\n",
            "Epoch 118/200\n",
            "390/390 - 59s - loss: 1.5029 - accuracy: 0.7815 - val_loss: 2.1699 - val_accuracy: 0.6614\n",
            "Epoch 119/200\n",
            "390/390 - 59s - loss: 1.4930 - accuracy: 0.7831 - val_loss: 2.0866 - val_accuracy: 0.6759\n",
            "Epoch 120/200\n",
            "390/390 - 59s - loss: 1.4968 - accuracy: 0.7801 - val_loss: 2.1481 - val_accuracy: 0.6654\n",
            "Epoch 121/200\n",
            "390/390 - 59s - loss: 1.4112 - accuracy: 0.8021 - val_loss: 2.0723 - val_accuracy: 0.6819\n",
            "Epoch 122/200\n",
            "390/390 - 59s - loss: 1.3779 - accuracy: 0.8100 - val_loss: 2.0562 - val_accuracy: 0.6840\n",
            "Epoch 123/200\n",
            "390/390 - 59s - loss: 1.3526 - accuracy: 0.8148 - val_loss: 2.0563 - val_accuracy: 0.6806\n",
            "Epoch 124/200\n",
            "390/390 - 59s - loss: 1.3368 - accuracy: 0.8180 - val_loss: 2.1064 - val_accuracy: 0.6842\n",
            "Epoch 125/200\n",
            "390/390 - 59s - loss: 1.3255 - accuracy: 0.8193 - val_loss: 2.0516 - val_accuracy: 0.6853\n",
            "Epoch 126/200\n",
            "390/390 - 59s - loss: 1.3167 - accuracy: 0.8202 - val_loss: 2.1440 - val_accuracy: 0.6774\n",
            "Epoch 127/200\n",
            "390/390 - 59s - loss: 1.3031 - accuracy: 0.8235 - val_loss: 2.0849 - val_accuracy: 0.6849\n",
            "Epoch 128/200\n",
            "390/390 - 59s - loss: 1.2914 - accuracy: 0.8287 - val_loss: 2.0805 - val_accuracy: 0.6832\n",
            "Epoch 129/200\n",
            "390/390 - 59s - loss: 1.2894 - accuracy: 0.8281 - val_loss: 2.1106 - val_accuracy: 0.6798\n",
            "Epoch 130/200\n",
            "390/390 - 59s - loss: 1.2670 - accuracy: 0.8318 - val_loss: 2.1231 - val_accuracy: 0.6812\n",
            "Epoch 131/200\n",
            "390/390 - 59s - loss: 1.2666 - accuracy: 0.8298 - val_loss: 2.0896 - val_accuracy: 0.6849\n",
            "Epoch 132/200\n",
            "390/390 - 59s - loss: 1.2633 - accuracy: 0.8307 - val_loss: 2.0806 - val_accuracy: 0.6826\n",
            "Epoch 133/200\n",
            "390/390 - 59s - loss: 1.2541 - accuracy: 0.8322 - val_loss: 2.0659 - val_accuracy: 0.6849\n",
            "Epoch 134/200\n",
            "390/390 - 59s - loss: 1.2357 - accuracy: 0.8368 - val_loss: 2.1191 - val_accuracy: 0.6798\n",
            "Epoch 135/200\n",
            "390/390 - 59s - loss: 1.2368 - accuracy: 0.8335 - val_loss: 2.0942 - val_accuracy: 0.6838\n",
            "Epoch 136/200\n",
            "390/390 - 59s - loss: 1.2286 - accuracy: 0.8348 - val_loss: 2.1254 - val_accuracy: 0.6761\n",
            "Epoch 137/200\n",
            "390/390 - 59s - loss: 1.2292 - accuracy: 0.8341 - val_loss: 2.0913 - val_accuracy: 0.6842\n",
            "Epoch 138/200\n",
            "390/390 - 59s - loss: 1.2174 - accuracy: 0.8359 - val_loss: 2.0916 - val_accuracy: 0.6818\n",
            "Epoch 139/200\n",
            "390/390 - 59s - loss: 1.2164 - accuracy: 0.8368 - val_loss: 2.0568 - val_accuracy: 0.6886\n",
            "Epoch 140/200\n",
            "390/390 - 59s - loss: 1.2070 - accuracy: 0.8379 - val_loss: 2.1162 - val_accuracy: 0.6803\n",
            "Epoch 141/200\n",
            "390/390 - 59s - loss: 1.1727 - accuracy: 0.8460 - val_loss: 2.0634 - val_accuracy: 0.6884\n",
            "Epoch 142/200\n",
            "390/390 - 59s - loss: 1.1341 - accuracy: 0.8577 - val_loss: 2.0490 - val_accuracy: 0.6885\n",
            "Epoch 143/200\n",
            "390/390 - 59s - loss: 1.1336 - accuracy: 0.8567 - val_loss: 2.0495 - val_accuracy: 0.6898\n",
            "Epoch 144/200\n",
            "390/390 - 59s - loss: 1.1228 - accuracy: 0.8605 - val_loss: 2.0511 - val_accuracy: 0.6924\n",
            "Epoch 145/200\n",
            "390/390 - 59s - loss: 1.1030 - accuracy: 0.8643 - val_loss: 2.0520 - val_accuracy: 0.6951\n",
            "Epoch 146/200\n",
            "390/390 - 59s - loss: 1.1015 - accuracy: 0.8632 - val_loss: 2.0556 - val_accuracy: 0.6935\n",
            "Epoch 147/200\n",
            "390/390 - 59s - loss: 1.1051 - accuracy: 0.8621 - val_loss: 2.0413 - val_accuracy: 0.6929\n",
            "Epoch 148/200\n",
            "390/390 - 59s - loss: 1.0749 - accuracy: 0.8707 - val_loss: 2.0893 - val_accuracy: 0.6868\n",
            "Epoch 149/200\n",
            "390/390 - 59s - loss: 1.0835 - accuracy: 0.8682 - val_loss: 2.0435 - val_accuracy: 0.6918\n",
            "Epoch 150/200\n",
            "390/390 - 59s - loss: 1.0731 - accuracy: 0.8683 - val_loss: 2.0882 - val_accuracy: 0.6885\n",
            "Epoch 151/200\n",
            "390/390 - 59s - loss: 1.0589 - accuracy: 0.8734 - val_loss: 2.1079 - val_accuracy: 0.6876\n",
            "Epoch 152/200\n",
            "390/390 - 59s - loss: 1.0619 - accuracy: 0.8706 - val_loss: 2.0577 - val_accuracy: 0.6935\n",
            "Epoch 153/200\n",
            "390/390 - 59s - loss: 1.0523 - accuracy: 0.8737 - val_loss: 2.0771 - val_accuracy: 0.6899\n",
            "Epoch 154/200\n",
            "390/390 - 59s - loss: 1.0525 - accuracy: 0.8731 - val_loss: 2.0988 - val_accuracy: 0.6861\n",
            "Epoch 155/200\n",
            "390/390 - 59s - loss: 1.0460 - accuracy: 0.8726 - val_loss: 2.0655 - val_accuracy: 0.6957\n",
            "Epoch 156/200\n",
            "390/390 - 59s - loss: 1.0341 - accuracy: 0.8764 - val_loss: 2.0940 - val_accuracy: 0.6896\n",
            "Epoch 157/200\n",
            "390/390 - 59s - loss: 1.0360 - accuracy: 0.8755 - val_loss: 2.0761 - val_accuracy: 0.6942\n",
            "Epoch 158/200\n",
            "390/390 - 59s - loss: 1.0288 - accuracy: 0.8765 - val_loss: 2.1021 - val_accuracy: 0.6863\n",
            "Epoch 159/200\n",
            "390/390 - 59s - loss: 1.0267 - accuracy: 0.8765 - val_loss: 2.0779 - val_accuracy: 0.6899\n",
            "Epoch 160/200\n",
            "390/390 - 59s - loss: 1.0274 - accuracy: 0.8761 - val_loss: 2.0728 - val_accuracy: 0.6906\n",
            "Epoch 161/200\n",
            "390/390 - 59s - loss: 1.0005 - accuracy: 0.8841 - val_loss: 2.0768 - val_accuracy: 0.6930\n",
            "Epoch 162/200\n",
            "390/390 - 59s - loss: 0.9915 - accuracy: 0.8864 - val_loss: 2.0904 - val_accuracy: 0.6928\n",
            "Epoch 163/200\n",
            "390/390 - 59s - loss: 0.9745 - accuracy: 0.8891 - val_loss: 2.0918 - val_accuracy: 0.6920\n",
            "Epoch 164/200\n",
            "390/390 - 59s - loss: 0.9756 - accuracy: 0.8882 - val_loss: 2.0599 - val_accuracy: 0.6929\n",
            "Epoch 165/200\n",
            "390/390 - 59s - loss: 0.9727 - accuracy: 0.8893 - val_loss: 2.0973 - val_accuracy: 0.6903\n",
            "Epoch 166/200\n",
            "390/390 - 59s - loss: 0.9718 - accuracy: 0.8899 - val_loss: 2.0824 - val_accuracy: 0.6930\n",
            "Epoch 167/200\n",
            "390/390 - 59s - loss: 0.9588 - accuracy: 0.8931 - val_loss: 2.0609 - val_accuracy: 0.6962\n",
            "Epoch 168/200\n",
            "390/390 - 59s - loss: 0.9553 - accuracy: 0.8925 - val_loss: 2.0724 - val_accuracy: 0.6947\n",
            "Epoch 169/200\n",
            "390/390 - 59s - loss: 0.9548 - accuracy: 0.8924 - val_loss: 2.0779 - val_accuracy: 0.6950\n",
            "Epoch 170/200\n",
            "390/390 - 59s - loss: 0.9498 - accuracy: 0.8960 - val_loss: 2.0966 - val_accuracy: 0.6918\n",
            "Epoch 171/200\n",
            "390/390 - 59s - loss: 0.9454 - accuracy: 0.8956 - val_loss: 2.0807 - val_accuracy: 0.6950\n",
            "Epoch 172/200\n",
            "390/390 - 59s - loss: 0.9466 - accuracy: 0.8952 - val_loss: 2.1031 - val_accuracy: 0.6899\n",
            "Epoch 173/200\n",
            "390/390 - 59s - loss: 0.9405 - accuracy: 0.8967 - val_loss: 2.1097 - val_accuracy: 0.6869\n",
            "Epoch 174/200\n",
            "390/390 - 59s - loss: 0.9325 - accuracy: 0.8979 - val_loss: 2.0784 - val_accuracy: 0.6949\n",
            "Epoch 175/200\n",
            "390/390 - 59s - loss: 0.9355 - accuracy: 0.8972 - val_loss: 2.0582 - val_accuracy: 0.6957\n",
            "Epoch 176/200\n",
            "390/390 - 59s - loss: 0.9310 - accuracy: 0.8982 - val_loss: 2.0941 - val_accuracy: 0.6900\n",
            "Epoch 177/200\n",
            "390/390 - 59s - loss: 0.9237 - accuracy: 0.8994 - val_loss: 2.0883 - val_accuracy: 0.6913\n",
            "Epoch 178/200\n",
            "390/390 - 59s - loss: 0.9242 - accuracy: 0.8977 - val_loss: 2.0746 - val_accuracy: 0.6928\n",
            "Epoch 179/200\n",
            "390/390 - 59s - loss: 0.9161 - accuracy: 0.9011 - val_loss: 2.1110 - val_accuracy: 0.6910\n",
            "Epoch 180/200\n",
            "390/390 - 59s - loss: 0.9152 - accuracy: 0.9004 - val_loss: 2.1314 - val_accuracy: 0.6887\n",
            "Epoch 181/200\n",
            "390/390 - 59s - loss: 0.9108 - accuracy: 0.9030 - val_loss: 2.0789 - val_accuracy: 0.6940\n",
            "Epoch 182/200\n",
            "390/390 - 59s - loss: 0.9009 - accuracy: 0.9055 - val_loss: 2.0808 - val_accuracy: 0.6925\n",
            "Epoch 183/200\n",
            "390/390 - 59s - loss: 0.8971 - accuracy: 0.9047 - val_loss: 2.0751 - val_accuracy: 0.6946\n",
            "Epoch 184/200\n",
            "390/390 - 59s - loss: 0.9038 - accuracy: 0.9029 - val_loss: 2.0859 - val_accuracy: 0.6929\n",
            "Epoch 185/200\n",
            "390/390 - 59s - loss: 0.8920 - accuracy: 0.9077 - val_loss: 2.0924 - val_accuracy: 0.6918\n",
            "Epoch 186/200\n",
            "390/390 - 59s - loss: 0.8941 - accuracy: 0.9069 - val_loss: 2.0752 - val_accuracy: 0.6945\n",
            "Epoch 187/200\n",
            "390/390 - 59s - loss: 0.8872 - accuracy: 0.9084 - val_loss: 2.0865 - val_accuracy: 0.6926\n",
            "Epoch 188/200\n",
            "390/390 - 59s - loss: 0.8839 - accuracy: 0.9067 - val_loss: 2.0986 - val_accuracy: 0.6916\n",
            "Epoch 189/200\n",
            "390/390 - 59s - loss: 0.8772 - accuracy: 0.9115 - val_loss: 2.0970 - val_accuracy: 0.6909\n",
            "Epoch 190/200\n",
            "390/390 - 59s - loss: 0.8805 - accuracy: 0.9077 - val_loss: 2.0955 - val_accuracy: 0.6911\n",
            "Epoch 191/200\n",
            "390/390 - 59s - loss: 0.8744 - accuracy: 0.9098 - val_loss: 2.1005 - val_accuracy: 0.6929\n",
            "Epoch 192/200\n",
            "390/390 - 59s - loss: 0.8786 - accuracy: 0.9088 - val_loss: 2.0958 - val_accuracy: 0.6939\n",
            "Epoch 193/200\n",
            "390/390 - 59s - loss: 0.8747 - accuracy: 0.9110 - val_loss: 2.0897 - val_accuracy: 0.6954\n",
            "Epoch 194/200\n",
            "390/390 - 59s - loss: 0.8682 - accuracy: 0.9106 - val_loss: 2.0749 - val_accuracy: 0.6957\n",
            "Epoch 195/200\n",
            "390/390 - 59s - loss: 0.8686 - accuracy: 0.9134 - val_loss: 2.0771 - val_accuracy: 0.6928\n",
            "Epoch 196/200\n",
            "390/390 - 59s - loss: 0.8732 - accuracy: 0.9092 - val_loss: 2.0990 - val_accuracy: 0.6938\n",
            "Epoch 197/200\n",
            "390/390 - 59s - loss: 0.8641 - accuracy: 0.9121 - val_loss: 2.1035 - val_accuracy: 0.6930\n",
            "Epoch 198/200\n",
            "390/390 - 59s - loss: 0.8689 - accuracy: 0.9098 - val_loss: 2.1052 - val_accuracy: 0.6911\n",
            "Epoch 199/200\n",
            "390/390 - 59s - loss: 0.8623 - accuracy: 0.9121 - val_loss: 2.1027 - val_accuracy: 0.6910\n",
            "Epoch 200/200\n",
            "390/390 - 59s - loss: 0.8586 - accuracy: 0.9126 - val_loss: 2.0937 - val_accuracy: 0.6947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7mM-AiYstvj"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}