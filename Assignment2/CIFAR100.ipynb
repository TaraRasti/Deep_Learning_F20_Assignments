{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (48000, 32, 32, 3)\n",
      "Train labels shape:  (48000,)\n",
      "Validation data shape:  (2000, 32, 32, 3)\n",
      "Validation labels shape:  (2000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.layers.core import Dense,Activation,Dropout,Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras import *\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import SGD\n",
    "import os\n",
    "from six.moves import cPickle as pickle\n",
    "import platform\n",
    "import numpy as np\n",
    " \n",
    "def load_files(filenames):\n",
    "    data = np.array([])\n",
    "    labels = np.array([])\n",
    "    for name in filenames:\n",
    "        with open(name, 'rb') as f:\n",
    "            mydict = pickle.load(f, encoding='latin1')\n",
    "\n",
    "        # The labels have different names in the two datasets.\n",
    "        label_func = lambda x: np.array(x['fine_labels'], dtype='int32')\n",
    "        newlabels = label_func(mydict)\n",
    "        if data.size:\n",
    "            data = np.vstack([data, mydict['data']])\n",
    "            labels = np.hstack([labels, newlabels])\n",
    "        else:\n",
    "            data = mydict['data']\n",
    "            labels = newlabels\n",
    "    data = np.reshape(data, [-1, 3, 32, 32], order='C')\n",
    "    data = np.transpose(data, [0, 2, 3, 1])\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def load_CIFAR100(data_dir):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    train_files = ['train']\n",
    "    train_files = [os.path.join(data_dir, f) for f in train_files]\n",
    "    test_files = ['test']\n",
    "    test_files = [os.path.join(data_dir, f) for f in test_files]\n",
    "    num_classes = 100\n",
    "    train_data, train_labels = load_files(train_files)\n",
    "    test_data, test_labels = load_files(test_files)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    " \n",
    "def get_CIFAR100_data(val_size=2000):\n",
    "    # Load the raw CIFAR-100 data\n",
    "    cifar100_dir = '/Users/taraapple/Desktop/cifar-100-python'\n",
    "    x_train, y_train, x_test, y_test = load_CIFAR100(cifar100_dir)\n",
    "    # Subsample the data\n",
    " \n",
    "    x_train, x_val = np.split(x_train,\n",
    "                              [x_train.shape[0]-val_size])\n",
    "    y_train, y_val = np.split(y_train,\n",
    "                              [y_train.shape[0]-val_size])\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    " \n",
    "    x_train /= 255\n",
    "    x_val /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    " \n",
    "# Invoke the above function to get our data.\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_CIFAR100_data()\n",
    " \n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', x_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 train samples, 32 channels, 32x3\n",
      "10000  test samples, 32 channels, 32x3\n",
      "2000  val samples, 32 channels, 32x3\n",
      "Epoch 1/12\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 4.4579 - accuracy: 0.0304 - val_loss: 4.1327 - val_accuracy: 0.0745\n",
      "Epoch 2/12\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 4.0024 - accuracy: 0.0858 - val_loss: 3.8193 - val_accuracy: 0.1335\n",
      "Epoch 3/12\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 3.7316 - accuracy: 0.1272 - val_loss: 3.5078 - val_accuracy: 0.1745\n",
      "Epoch 4/12\n",
      "375/375 [==============================] - 68s 183ms/step - loss: 3.5091 - accuracy: 0.1664 - val_loss: 3.3296 - val_accuracy: 0.1955\n",
      "Epoch 5/12\n",
      "375/375 [==============================] - 69s 183ms/step - loss: 3.3187 - accuracy: 0.2018 - val_loss: 3.1525 - val_accuracy: 0.2310\n",
      "Epoch 6/12\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 3.1460 - accuracy: 0.2340 - val_loss: 2.9369 - val_accuracy: 0.2815\n",
      "Epoch 7/12\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 2.9893 - accuracy: 0.2639 - val_loss: 2.7749 - val_accuracy: 0.3115\n",
      "Epoch 8/12\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 2.8533 - accuracy: 0.2886 - val_loss: 2.7440 - val_accuracy: 0.3210\n",
      "Epoch 9/12\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 2.7248 - accuracy: 0.3121 - val_loss: 2.6289 - val_accuracy: 0.3400\n",
      "Epoch 10/12\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 2.6119 - accuracy: 0.3343 - val_loss: 2.5012 - val_accuracy: 0.3655\n",
      "Epoch 11/12\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 2.5019 - accuracy: 0.3590 - val_loss: 2.4511 - val_accuracy: 0.3790\n",
      "Epoch 12/12\n",
      "375/375 [==============================] - 69s 184ms/step - loss: 2.4048 - accuracy: 0.3762 - val_loss: 2.3774 - val_accuracy: 0.3915\n",
      "Test score: 2.318232536315918\n",
      "Test accuracy: 0.4036000072956085\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# building model with too many classes inially for simpler transfer learning afterwards\n",
    "nb_classes = 100\n",
    "nb_epoch = 12\n",
    "\n",
    "\n",
    "# print shape of data while model is building\n",
    "print(\"{1} train samples, {2} channel{0}, {3}x{4}\".format(\"\" if x_train.shape[1] == 1 else \"s\", *x_train.shape))\n",
    "print(\"{1}  test samples, {2} channel{0}, {3}x{4}\".format(\"\" if x_test.shape[1] == 1 else \"s\", *x_test.shape))\n",
    "print(\"{1}  val samples, {2} channel{0}, {3}x{4}\".format(\"\" if x_val.shape[1] == 1 else \"s\", *x_val.shape))\n",
    "# input image dimensions\n",
    "_, img_channels, img_rows, img_cols = x_train.shape\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Conv2D(32,(3,3),padding='same',input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32,(3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Conv2D(64,(3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,(3,3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100,activation='softmax'))\n",
    "# let's train the model using SGD + momentum (how original).\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=12, verbose=1, validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "375/375 [==============================] - 68s 180ms/step - loss: 2.3172 - accuracy: 0.3979 - val_loss: 2.3553 - val_accuracy: 0.4050\n",
      "Epoch 2/12\n",
      "375/375 [==============================] - 69s 185ms/step - loss: 2.2280 - accuracy: 0.4143 - val_loss: 2.3078 - val_accuracy: 0.4175\n",
      "Epoch 3/12\n",
      "375/375 [==============================] - 70s 187ms/step - loss: 2.1500 - accuracy: 0.4322 - val_loss: 2.2653 - val_accuracy: 0.4215\n",
      "Epoch 4/12\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 2.0687 - accuracy: 0.4495 - val_loss: 2.2179 - val_accuracy: 0.4215\n",
      "Epoch 5/12\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.9988 - accuracy: 0.4595 - val_loss: 2.1730 - val_accuracy: 0.4430\n",
      "Epoch 6/12\n",
      "375/375 [==============================] - 71s 189ms/step - loss: 1.9317 - accuracy: 0.4749 - val_loss: 2.1778 - val_accuracy: 0.4360\n",
      "Epoch 7/12\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.8634 - accuracy: 0.4925 - val_loss: 2.1507 - val_accuracy: 0.4470\n",
      "Epoch 8/12\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.7960 - accuracy: 0.5063 - val_loss: 2.1564 - val_accuracy: 0.4480\n",
      "Epoch 9/12\n",
      "375/375 [==============================] - 71s 190ms/step - loss: 1.7376 - accuracy: 0.5201 - val_loss: 2.1212 - val_accuracy: 0.4575\n",
      "Epoch 10/12\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.6778 - accuracy: 0.5326 - val_loss: 2.1433 - val_accuracy: 0.4540\n",
      "Epoch 11/12\n",
      "375/375 [==============================] - 72s 191ms/step - loss: 1.6213 - accuracy: 0.5460 - val_loss: 2.1163 - val_accuracy: 0.4505\n",
      "Epoch 12/12\n",
      "375/375 [==============================] - 72s 192ms/step - loss: 1.5733 - accuracy: 0.5552 - val_loss: 2.1385 - val_accuracy: 0.4540\n",
      "Test score: 2.073385000228882\n",
      "Test accuracy: 0.4675999879837036\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128, epochs=12, verbose=1, validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
