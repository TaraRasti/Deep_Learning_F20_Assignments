{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = [[1,1,0,0,1,1,1,1],[1,1,0,0,0,1,1,1],[1,0,0,0,1,1,1,1],[0,1,1,1,0,0,0,1],[0,0,0,1,0,0,0,0],[0,0,0,1,1,0,0,0]]\n",
    "Y = [1,1,1,0,0,0]\n",
    "labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_net(layer_array, input_dims):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    activations = []\n",
    "    \n",
    "    for i in range(len(layer_array)):\n",
    "        node_num = layer_array[i][0]\n",
    "        weights_of_layer = []\n",
    "        biases_of_layer = []\n",
    "        if i == 0:\n",
    "            last_layer_node_number = input_dims\n",
    "        else:\n",
    "            last_layer_node_number = layer_array[i-1][0]\n",
    "        \n",
    "        for n in range(0,node_num):\n",
    "            weights_of_node = []\n",
    "            for l in range(0, last_layer_node_number):\n",
    "                weights_of_node.append(1) \n",
    "            weights_of_layer.append(weights_of_node)\n",
    "            biases_of_layer.append(0)\n",
    "            \n",
    "        weights.append(weights_of_layer)\n",
    "        biases.append(biases_of_layer)\n",
    "        activations.append(layer_array[i][1])\n",
    "    return [weights, biases, activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " weights: [[[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]] \n",
      "\n",
      " biases: [[0, 0]] \n",
      "\n",
      " activations: ['sigmoid']\n"
     ]
    }
   ],
   "source": [
    "layer_array = [[len(labels), 'sigmoid']]\n",
    "input_dims = 8\n",
    "neural_net = create_neural_net(layer_array, input_dims)\n",
    "\n",
    "print(' weights:',neural_net[0],'\\n\\n biases:',neural_net[1],'\\n\\n activations:', neural_net[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratio(data, neural_net):\n",
    "    weights = neural_net[0]\n",
    "    biases = neural_net[1]\n",
    "    activations = neural_net[2]\n",
    "    \n",
    "    layer_num = len(weights)\n",
    "    \n",
    "    for l in range(0, layer_num):\n",
    "        data = np.dot(weights[l], data)\n",
    "        for t in range(len(data)):\n",
    "            data[t] += biases[l][t]\n",
    "        if activations[l] == 'sigmoid':\n",
    "            data = sigmoid(data)\n",
    "        elif activations[l] == 'relu':\n",
    "            data = relu(data)\n",
    "        else:\n",
    "            data = sigmoid(data)\n",
    "            print('activation function', activations[l], 'cannot be found. Sigmoid is used')   \n",
    "    return data\n",
    "\n",
    "def predict(data, neural_net):\n",
    "    data = predict_ratio(data, neural_net)\n",
    "    \n",
    "    class_num = len(data)\n",
    "    \n",
    "    highest_class = None\n",
    "    highest_class_probability = -1\n",
    "    \n",
    "    for i in range(0, class_num):\n",
    "        if highest_class == None:\n",
    "            highest_class = i\n",
    "            highest_class_probability = data[i]\n",
    "        elif data[i] > highest_class_probability:\n",
    "            highest_class = i\n",
    "            highest_class_probability = data[i]\n",
    "            \n",
    "    return highest_class, highest_class_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, Y, labels, neural_net, epochs=1000):\n",
    "    for epoch in range(0, epochs):\n",
    "        for d in range(0, len(X)):\n",
    "            prediction = predict_ratio(X[d], neural_net)\n",
    "            true_prediction = []\n",
    "            for i in range(0, len(labels)):\n",
    "                true_prediction.append(0)\n",
    "            true_prediction[labels.index(Y[d])] = 1\n",
    "            \n",
    "            errors = []\n",
    "            for t in range(len(prediction)):\n",
    "                errors.append(true_prediction[t] - prediction[t]) \n",
    "            adjust_deriv = errors * sigmoid_deriv(prediction)\n",
    "            \n",
    "            for k in range(0, len(adjust_deriv)):\n",
    "                adjustment = np.dot(X[d], adjust_deriv[k])\n",
    "                neural_net[0][0][k] += adjustment\n",
    "    return neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = train_network(X, Y, labels, neural_net, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.9919398375371878)\n",
      "(1, 0.9936373425420446)\n",
      "(1, 0.9925307416847557)\n",
      "(0, 0.9903682943291514)\n",
      "(0, 0.9836167677309535)\n",
      "(0, 0.9876082070557368)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    print(predict(X[i], neural_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model with MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "x = (x/255).astype('float32')\n",
    "y = to_categorical(y)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 32.31s, Accuracy: 94.5\n",
      "Epoch: 2, Time Spent: 66.70s, Accuracy: 94.5\n",
      "Epoch: 3, Time Spent: 101.08s, Accuracy: 94.5\n",
      "Epoch: 4, Time Spent: 134.38s, Accuracy: 94.5\n",
      "Epoch: 5, Time Spent: 171.25s, Accuracy: 94.5\n",
      "Epoch: 6, Time Spent: 206.02s, Accuracy: 94.5\n",
      "Epoch: 7, Time Spent: 238.57s, Accuracy: 94.5\n",
      "Epoch: 8, Time Spent: 273.84s, Accuracy: 94.5\n",
      "Epoch: 9, Time Spent: 310.04s, Accuracy: 94.5\n",
      "Epoch: 10, Time Spent: 344.32s, Accuracy: 94.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNeuralNetwork():\n",
    "    def __init__(self, sizes, epochs=10, l_rate=0.001):\n",
    "        self.sizes = sizes\n",
    "        self.epochs = epochs\n",
    "        self.l_rate = l_rate\n",
    "        self.params = self.initialization()\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - x.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def initialization(self):\n",
    "        input_layer=self.sizes[0]\n",
    "        hidden_1=self.sizes[1]\n",
    "        hidden_2=self.sizes[2]\n",
    "        output_layer=self.sizes[3]\n",
    "\n",
    "        params = {\n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3':np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "        params['A0'] = x_train\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\n",
    "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
    "        params['A3'] = self.softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "\n",
    "    def backward_pass(self, y_train, output):\n",
    "   \n",
    "\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        error = output - y_train\n",
    "        change_w['W3'] = np.dot(error, params['A3'])\n",
    "\n",
    "        error = np.multiply( np.dot(params['W3'].T, error), self.sigmoid(params['Z2'], derivative=True) )\n",
    "        change_w['W2'] = np.dot(error, params['A2'])\n",
    "\n",
    "        error = np.multiply( np.dot(params['W2'].T, error), self.sigmoid(params['Z1'], derivative=True) )\n",
    "        change_w['W1'] = np.dot(error, params['A1'])\n",
    "\n",
    "        return change_w\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        for key, value in changes_to_w.items():\n",
    "            for w_arr in self.params[key]:\n",
    "                w_arr -= self.l_rate * value\n",
    "\n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        predictions = []\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == y)\n",
    "        \n",
    "        summed = sum(pred for pred in predictions) / 100.0\n",
    "        return np.average(summed)\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        start_time = time.time()\n",
    "        for iteration in range(self.epochs):\n",
    "            for x,y in zip(x_train, y_train):\n",
    "                output = self.forward_pass(x)\n",
    "                changes_to_w = self.backward_pass(y, output)\n",
    "                self.update_network_parameters(changes_to_w)\n",
    "            \n",
    "            accuracy = self.compute_accuracy(x_val, y_val)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2}'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy\n",
    "            ))\n",
    "            \n",
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])\n",
    "dnn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
