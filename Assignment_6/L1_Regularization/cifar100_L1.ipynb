{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar100_L1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6SWIKqYg/dVy0wDHq4ZeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaraRasti/Deep_Learning_F20_Assignments/blob/master/Assignment_6/L1_Regularization/cifar100_L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0b6rqoXYARr",
        "outputId": "97b9ada0-2065-43c8-bc57-38b7dd1f8272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar100\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 100\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "baseMapNum = 32\n",
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    samplewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "#training\n",
        "batch_size = 64\n",
        "epochs=25\n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=opt_rms,\n",
        "        metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=3*epochs,verbose=1,validation_data=(x_test,y_test))\n",
        "model.save_weights('cifar100_normal_rms_ep75.h5')\n",
        "\n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.0005,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=opt_rms,\n",
        "        metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
        "model.save_weights('cifar100_normal_rms_ep100.h5')\n",
        "\n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.0003,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=opt_rms,\n",
        "        metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
        "model.save_weights('cifar100_normal_rms_ep125.h5')\n",
        "\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               204900    \n",
            "=================================================================\n",
            "Total params: 493,700\n",
            "Trainable params: 492,804\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Epoch 1/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 5.1759 - accuracy: 0.1230 - val_loss: 3.9683 - val_accuracy: 0.2215\n",
            "Epoch 2/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 4.2458 - accuracy: 0.2192 - val_loss: 3.6932 - val_accuracy: 0.2861\n",
            "Epoch 3/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 3.8150 - accuracy: 0.2706 - val_loss: 3.3691 - val_accuracy: 0.3255\n",
            "Epoch 4/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 3.4419 - accuracy: 0.3107 - val_loss: 3.1576 - val_accuracy: 0.3693\n",
            "Epoch 5/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 3.2091 - accuracy: 0.3443 - val_loss: 2.9913 - val_accuracy: 0.3996\n",
            "Epoch 6/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 3.0866 - accuracy: 0.3720 - val_loss: 2.9045 - val_accuracy: 0.4145\n",
            "Epoch 7/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.9889 - accuracy: 0.3961 - val_loss: 2.9298 - val_accuracy: 0.4175\n",
            "Epoch 8/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.9377 - accuracy: 0.4073 - val_loss: 2.8055 - val_accuracy: 0.4489\n",
            "Epoch 9/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.8941 - accuracy: 0.4178 - val_loss: 2.8061 - val_accuracy: 0.4460\n",
            "Epoch 10/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.8459 - accuracy: 0.4326 - val_loss: 2.7739 - val_accuracy: 0.4575\n",
            "Epoch 11/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.8236 - accuracy: 0.4377 - val_loss: 2.7293 - val_accuracy: 0.4689\n",
            "Epoch 12/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.7938 - accuracy: 0.4459 - val_loss: 2.6332 - val_accuracy: 0.4984\n",
            "Epoch 13/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.7787 - accuracy: 0.4515 - val_loss: 2.8245 - val_accuracy: 0.4636\n",
            "Epoch 14/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.7592 - accuracy: 0.4566 - val_loss: 2.7758 - val_accuracy: 0.4659\n",
            "Epoch 15/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.7458 - accuracy: 0.4621 - val_loss: 2.6444 - val_accuracy: 0.4985\n",
            "Epoch 16/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.7194 - accuracy: 0.4686 - val_loss: 2.8604 - val_accuracy: 0.4586\n",
            "Epoch 17/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.7120 - accuracy: 0.4700 - val_loss: 2.7043 - val_accuracy: 0.4891\n",
            "Epoch 18/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.7103 - accuracy: 0.4725 - val_loss: 2.7312 - val_accuracy: 0.4873\n",
            "Epoch 19/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.6955 - accuracy: 0.4758 - val_loss: 2.6298 - val_accuracy: 0.5033\n",
            "Epoch 20/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6937 - accuracy: 0.4763 - val_loss: 2.6565 - val_accuracy: 0.4963\n",
            "Epoch 21/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.6800 - accuracy: 0.4826 - val_loss: 2.6506 - val_accuracy: 0.4982\n",
            "Epoch 22/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6696 - accuracy: 0.4871 - val_loss: 2.5819 - val_accuracy: 0.5133\n",
            "Epoch 23/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6552 - accuracy: 0.4879 - val_loss: 2.7063 - val_accuracy: 0.4953\n",
            "Epoch 24/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6475 - accuracy: 0.4930 - val_loss: 2.5506 - val_accuracy: 0.5213\n",
            "Epoch 25/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.6474 - accuracy: 0.4929 - val_loss: 2.6158 - val_accuracy: 0.5152\n",
            "Epoch 26/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6411 - accuracy: 0.4940 - val_loss: 2.5824 - val_accuracy: 0.5193\n",
            "Epoch 27/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.6358 - accuracy: 0.4943 - val_loss: 2.4948 - val_accuracy: 0.5332\n",
            "Epoch 28/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6432 - accuracy: 0.4949 - val_loss: 2.5675 - val_accuracy: 0.5265\n",
            "Epoch 29/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.6255 - accuracy: 0.4983 - val_loss: 2.6234 - val_accuracy: 0.5102\n",
            "Epoch 30/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6264 - accuracy: 0.4990 - val_loss: 2.6319 - val_accuracy: 0.5049\n",
            "Epoch 31/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6223 - accuracy: 0.4993 - val_loss: 2.6040 - val_accuracy: 0.5186\n",
            "Epoch 32/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.6112 - accuracy: 0.5049 - val_loss: 2.5018 - val_accuracy: 0.5378\n",
            "Epoch 33/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.6113 - accuracy: 0.5045 - val_loss: 2.5146 - val_accuracy: 0.5389\n",
            "Epoch 34/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.6139 - accuracy: 0.5040 - val_loss: 2.6344 - val_accuracy: 0.5129\n",
            "Epoch 35/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.6040 - accuracy: 0.5073 - val_loss: 2.5017 - val_accuracy: 0.5330\n",
            "Epoch 36/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.5875 - accuracy: 0.5103 - val_loss: 2.4997 - val_accuracy: 0.5340\n",
            "Epoch 37/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.5969 - accuracy: 0.5066 - val_loss: 2.6676 - val_accuracy: 0.5114\n",
            "Epoch 38/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.5987 - accuracy: 0.5092 - val_loss: 2.4555 - val_accuracy: 0.5447\n",
            "Epoch 39/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5902 - accuracy: 0.5099 - val_loss: 2.5139 - val_accuracy: 0.5418\n",
            "Epoch 40/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.5962 - accuracy: 0.5092 - val_loss: 2.5851 - val_accuracy: 0.5269\n",
            "Epoch 41/75\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 2.5968 - accuracy: 0.5124 - val_loss: 2.5576 - val_accuracy: 0.5269\n",
            "Epoch 42/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.5839 - accuracy: 0.5126 - val_loss: 2.5950 - val_accuracy: 0.5175\n",
            "Epoch 43/75\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 2.5901 - accuracy: 0.5118 - val_loss: 2.6165 - val_accuracy: 0.5271\n",
            "Epoch 44/75\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 2.5764 - accuracy: 0.5147 - val_loss: 2.4327 - val_accuracy: 0.5470\n",
            "Epoch 45/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5795 - accuracy: 0.5147 - val_loss: 2.6849 - val_accuracy: 0.5094\n",
            "Epoch 46/75\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 2.5713 - accuracy: 0.5181 - val_loss: 2.6927 - val_accuracy: 0.5103\n",
            "Epoch 47/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5823 - accuracy: 0.5158 - val_loss: 2.5694 - val_accuracy: 0.5310\n",
            "Epoch 48/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5655 - accuracy: 0.5192 - val_loss: 2.4961 - val_accuracy: 0.5429\n",
            "Epoch 49/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5762 - accuracy: 0.5158 - val_loss: 2.5868 - val_accuracy: 0.5235\n",
            "Epoch 50/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5703 - accuracy: 0.5154 - val_loss: 2.5072 - val_accuracy: 0.5360\n",
            "Epoch 51/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5696 - accuracy: 0.5181 - val_loss: 2.5160 - val_accuracy: 0.5380\n",
            "Epoch 52/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5559 - accuracy: 0.5208 - val_loss: 2.5673 - val_accuracy: 0.5306\n",
            "Epoch 53/75\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 2.5646 - accuracy: 0.5205 - val_loss: 2.5423 - val_accuracy: 0.5363\n",
            "Epoch 54/75\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 2.5573 - accuracy: 0.5188 - val_loss: 2.5042 - val_accuracy: 0.5378\n",
            "Epoch 55/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 2.5660 - accuracy: 0.5188 - val_loss: 2.6335 - val_accuracy: 0.5077\n",
            "Epoch 56/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 2.5655 - accuracy: 0.5192 - val_loss: 2.5851 - val_accuracy: 0.5274\n",
            "Epoch 57/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5584 - accuracy: 0.5194 - val_loss: 2.4999 - val_accuracy: 0.5414\n",
            "Epoch 58/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5668 - accuracy: 0.5161 - val_loss: 2.5928 - val_accuracy: 0.5211\n",
            "Epoch 59/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5603 - accuracy: 0.5229 - val_loss: 2.4556 - val_accuracy: 0.5488\n",
            "Epoch 60/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5479 - accuracy: 0.5241 - val_loss: 2.4333 - val_accuracy: 0.5561\n",
            "Epoch 61/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.5494 - accuracy: 0.5201 - val_loss: 2.4780 - val_accuracy: 0.5419\n",
            "Epoch 62/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5448 - accuracy: 0.5252 - val_loss: 2.6120 - val_accuracy: 0.5205\n",
            "Epoch 63/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5442 - accuracy: 0.5244 - val_loss: 2.4413 - val_accuracy: 0.5496\n",
            "Epoch 64/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5404 - accuracy: 0.5242 - val_loss: 2.5161 - val_accuracy: 0.5294\n",
            "Epoch 65/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5445 - accuracy: 0.5240 - val_loss: 2.5972 - val_accuracy: 0.5315\n",
            "Epoch 66/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5433 - accuracy: 0.5268 - val_loss: 2.6105 - val_accuracy: 0.5233\n",
            "Epoch 67/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5402 - accuracy: 0.5239 - val_loss: 2.4581 - val_accuracy: 0.5460\n",
            "Epoch 68/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.5402 - accuracy: 0.5264 - val_loss: 2.4942 - val_accuracy: 0.5395\n",
            "Epoch 69/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5337 - accuracy: 0.5254 - val_loss: 2.4881 - val_accuracy: 0.5472\n",
            "Epoch 70/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5435 - accuracy: 0.5232 - val_loss: 2.5132 - val_accuracy: 0.5448\n",
            "Epoch 71/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.5360 - accuracy: 0.5278 - val_loss: 2.5820 - val_accuracy: 0.5286\n",
            "Epoch 72/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5310 - accuracy: 0.5278 - val_loss: 2.5360 - val_accuracy: 0.5328\n",
            "Epoch 73/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.5327 - accuracy: 0.5256 - val_loss: 2.5241 - val_accuracy: 0.5398\n",
            "Epoch 74/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.5346 - accuracy: 0.5241 - val_loss: 2.5422 - val_accuracy: 0.5406\n",
            "Epoch 75/75\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.5294 - accuracy: 0.5255 - val_loss: 2.5073 - val_accuracy: 0.5433\n",
            "Epoch 1/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.3750 - accuracy: 0.5587 - val_loss: 2.3509 - val_accuracy: 0.5694\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.3089 - accuracy: 0.5638 - val_loss: 2.2453 - val_accuracy: 0.5838\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2854 - accuracy: 0.5685 - val_loss: 2.3378 - val_accuracy: 0.5658\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2642 - accuracy: 0.5665 - val_loss: 2.2213 - val_accuracy: 0.5888\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2413 - accuracy: 0.5713 - val_loss: 2.2315 - val_accuracy: 0.5788\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2443 - accuracy: 0.5667 - val_loss: 2.2625 - val_accuracy: 0.5661\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2302 - accuracy: 0.5704 - val_loss: 2.1677 - val_accuracy: 0.5871\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2287 - accuracy: 0.5709 - val_loss: 2.2091 - val_accuracy: 0.5807\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 2.2266 - accuracy: 0.5693 - val_loss: 2.3378 - val_accuracy: 0.5561\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.2177 - accuracy: 0.5698 - val_loss: 2.1882 - val_accuracy: 0.5861\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 2.2138 - accuracy: 0.5730 - val_loss: 2.2312 - val_accuracy: 0.5732\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.2092 - accuracy: 0.5703 - val_loss: 2.2159 - val_accuracy: 0.5780\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.2024 - accuracy: 0.5733 - val_loss: 2.2368 - val_accuracy: 0.5689\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.2013 - accuracy: 0.5718 - val_loss: 2.1570 - val_accuracy: 0.5865\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1915 - accuracy: 0.5732 - val_loss: 2.1934 - val_accuracy: 0.5771\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1907 - accuracy: 0.5736 - val_loss: 2.2481 - val_accuracy: 0.5667\n",
            "Epoch 17/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 2.1891 - accuracy: 0.5710 - val_loss: 2.2728 - val_accuracy: 0.5648\n",
            "Epoch 18/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.2007 - accuracy: 0.5670 - val_loss: 2.1697 - val_accuracy: 0.5842\n",
            "Epoch 19/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1951 - accuracy: 0.5681 - val_loss: 2.2510 - val_accuracy: 0.5695\n",
            "Epoch 20/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1842 - accuracy: 0.5738 - val_loss: 2.2349 - val_accuracy: 0.5718\n",
            "Epoch 21/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1867 - accuracy: 0.5708 - val_loss: 2.2133 - val_accuracy: 0.5756\n",
            "Epoch 22/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1862 - accuracy: 0.5729 - val_loss: 2.1377 - val_accuracy: 0.5938\n",
            "Epoch 23/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1775 - accuracy: 0.5746 - val_loss: 2.2358 - val_accuracy: 0.5642\n",
            "Epoch 24/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1843 - accuracy: 0.5708 - val_loss: 2.2200 - val_accuracy: 0.5715\n",
            "Epoch 25/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.1840 - accuracy: 0.5736 - val_loss: 2.1659 - val_accuracy: 0.5766\n",
            "Epoch 1/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 2.1012 - accuracy: 0.5911 - val_loss: 2.1673 - val_accuracy: 0.5851\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.0591 - accuracy: 0.5968 - val_loss: 2.0553 - val_accuracy: 0.5990\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.0524 - accuracy: 0.5970 - val_loss: 2.1220 - val_accuracy: 0.5898\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.0393 - accuracy: 0.5961 - val_loss: 2.0338 - val_accuracy: 0.6018\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.0349 - accuracy: 0.5964 - val_loss: 2.1061 - val_accuracy: 0.5883\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.0243 - accuracy: 0.5987 - val_loss: 2.0963 - val_accuracy: 0.5883\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.0187 - accuracy: 0.5990 - val_loss: 2.0544 - val_accuracy: 0.6007\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 2.0088 - accuracy: 0.5986 - val_loss: 2.0688 - val_accuracy: 0.5902\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 2.0081 - accuracy: 0.6006 - val_loss: 2.0894 - val_accuracy: 0.5877\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9980 - accuracy: 0.6024 - val_loss: 2.0630 - val_accuracy: 0.5954\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9998 - accuracy: 0.5974 - val_loss: 2.1328 - val_accuracy: 0.5762\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9994 - accuracy: 0.5984 - val_loss: 2.0530 - val_accuracy: 0.5951\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9885 - accuracy: 0.6009 - val_loss: 2.1060 - val_accuracy: 0.5800\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9959 - accuracy: 0.5981 - val_loss: 2.0047 - val_accuracy: 0.6032\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9792 - accuracy: 0.6017 - val_loss: 2.0821 - val_accuracy: 0.5876\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9901 - accuracy: 0.5990 - val_loss: 2.0355 - val_accuracy: 0.5940\n",
            "Epoch 17/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9862 - accuracy: 0.5984 - val_loss: 2.0279 - val_accuracy: 0.5980\n",
            "Epoch 18/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9825 - accuracy: 0.5999 - val_loss: 2.0179 - val_accuracy: 0.5980\n",
            "Epoch 19/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9814 - accuracy: 0.6009 - val_loss: 2.0558 - val_accuracy: 0.5911\n",
            "Epoch 20/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9779 - accuracy: 0.6022 - val_loss: 2.1096 - val_accuracy: 0.5796\n",
            "Epoch 21/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9836 - accuracy: 0.5982 - val_loss: 2.0314 - val_accuracy: 0.5955\n",
            "Epoch 22/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9750 - accuracy: 0.5992 - val_loss: 2.1275 - val_accuracy: 0.5738\n",
            "Epoch 23/25\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 1.9780 - accuracy: 0.5982 - val_loss: 2.0416 - val_accuracy: 0.5942\n",
            "Epoch 24/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9794 - accuracy: 0.6010 - val_loss: 2.0599 - val_accuracy: 0.5938\n",
            "Epoch 25/25\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.9699 - accuracy: 0.6015 - val_loss: 2.0120 - val_accuracy: 0.5970\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 2.0120 - accuracy: 0.5970\n",
            "\n",
            "Test result: 59.700 loss: 2.012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9NML26fYNc8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}