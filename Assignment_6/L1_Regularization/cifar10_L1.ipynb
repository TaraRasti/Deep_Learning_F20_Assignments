{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_L1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4muTavI4pspPKibjV3GDu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaraRasti/Deep_Learning_F20_Assignments/blob/master/Assignment_6/L1_Regularization/cifar10_L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0b6rqoXYARr",
        "outputId": "830d4df9-81db-44d1-a23d-452d9107a7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "baseMapNum = 32\n",
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l1(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    samplewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False\n",
        "    )\n",
        "datagen.fit(x_train)\n",
        "\n",
        "#training\n",
        "batch_size = 64\n",
        "epochs=25\n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=opt_rms,\n",
        "        metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=3*epochs,verbose=1,validation_data=(x_test,y_test))\n",
        "model.save_weights('cifar10_normal_rms_ep75.h5')\n",
        "\n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.0005,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=opt_rms,\n",
        "        metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
        "model.save_weights('cifar10_normal_rms_ep100.h5')\n",
        "\n",
        "opt_rms = keras.optimizers.RMSprop(lr=0.0003,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=opt_rms,\n",
        "        metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))\n",
        "model.save_weights('cifar10_normal_rms_ep125.h5')\n",
        "\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From <ipython-input-1-82173e68ae65>:83: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 2.7094 - accuracy: 0.4243 - val_loss: 2.0494 - val_accuracy: 0.5461\n",
            "Epoch 2/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.9824 - accuracy: 0.5708 - val_loss: 1.7747 - val_accuracy: 0.6241\n",
            "Epoch 3/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.6751 - accuracy: 0.6350 - val_loss: 1.6237 - val_accuracy: 0.6485\n",
            "Epoch 4/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.5129 - accuracy: 0.6688 - val_loss: 1.5801 - val_accuracy: 0.6665\n",
            "Epoch 5/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.4252 - accuracy: 0.6909 - val_loss: 1.2922 - val_accuracy: 0.7332\n",
            "Epoch 6/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.3618 - accuracy: 0.7080 - val_loss: 1.3495 - val_accuracy: 0.7198\n",
            "Epoch 7/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.3061 - accuracy: 0.7230 - val_loss: 1.2788 - val_accuracy: 0.7422\n",
            "Epoch 8/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.2718 - accuracy: 0.7305 - val_loss: 1.2168 - val_accuracy: 0.7626\n",
            "Epoch 9/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.2359 - accuracy: 0.7423 - val_loss: 1.2697 - val_accuracy: 0.7415\n",
            "Epoch 10/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.2012 - accuracy: 0.7512 - val_loss: 1.1827 - val_accuracy: 0.7629\n",
            "Epoch 11/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.1780 - accuracy: 0.7566 - val_loss: 1.1003 - val_accuracy: 0.7853\n",
            "Epoch 12/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.1565 - accuracy: 0.7574 - val_loss: 1.0697 - val_accuracy: 0.7934\n",
            "Epoch 13/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.1350 - accuracy: 0.7659 - val_loss: 1.0213 - val_accuracy: 0.8079\n",
            "Epoch 14/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.1274 - accuracy: 0.7674 - val_loss: 1.1143 - val_accuracy: 0.7721\n",
            "Epoch 15/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 1.1041 - accuracy: 0.7729 - val_loss: 1.0505 - val_accuracy: 0.7935\n",
            "Epoch 16/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0969 - accuracy: 0.7742 - val_loss: 1.1036 - val_accuracy: 0.7830\n",
            "Epoch 17/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0902 - accuracy: 0.7761 - val_loss: 1.0448 - val_accuracy: 0.8001\n",
            "Epoch 18/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0805 - accuracy: 0.7813 - val_loss: 1.0897 - val_accuracy: 0.7874\n",
            "Epoch 19/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0730 - accuracy: 0.7817 - val_loss: 1.1094 - val_accuracy: 0.7743\n",
            "Epoch 20/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0631 - accuracy: 0.7808 - val_loss: 1.0765 - val_accuracy: 0.7863\n",
            "Epoch 21/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0526 - accuracy: 0.7871 - val_loss: 0.9766 - val_accuracy: 0.8170\n",
            "Epoch 22/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0449 - accuracy: 0.7869 - val_loss: 0.9972 - val_accuracy: 0.8113\n",
            "Epoch 23/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0423 - accuracy: 0.7887 - val_loss: 1.1029 - val_accuracy: 0.7786\n",
            "Epoch 24/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0417 - accuracy: 0.7895 - val_loss: 1.1032 - val_accuracy: 0.7764\n",
            "Epoch 25/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0334 - accuracy: 0.7899 - val_loss: 0.9595 - val_accuracy: 0.8192\n",
            "Epoch 26/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0253 - accuracy: 0.7941 - val_loss: 1.0115 - val_accuracy: 0.8023\n",
            "Epoch 27/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0246 - accuracy: 0.7920 - val_loss: 0.9587 - val_accuracy: 0.8135\n",
            "Epoch 28/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0258 - accuracy: 0.7927 - val_loss: 0.9104 - val_accuracy: 0.8320\n",
            "Epoch 29/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0173 - accuracy: 0.7953 - val_loss: 0.9594 - val_accuracy: 0.8145\n",
            "Epoch 30/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0138 - accuracy: 0.7948 - val_loss: 1.0287 - val_accuracy: 0.7891\n",
            "Epoch 31/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0116 - accuracy: 0.7950 - val_loss: 0.9439 - val_accuracy: 0.8217\n",
            "Epoch 32/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 1.0108 - accuracy: 0.7946 - val_loss: 0.9533 - val_accuracy: 0.8141\n",
            "Epoch 33/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0051 - accuracy: 0.7984 - val_loss: 0.9622 - val_accuracy: 0.8208\n",
            "Epoch 34/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0047 - accuracy: 0.7963 - val_loss: 0.9683 - val_accuracy: 0.8115\n",
            "Epoch 35/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.0032 - accuracy: 0.7969 - val_loss: 0.9636 - val_accuracy: 0.8136\n",
            "Epoch 36/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9978 - accuracy: 0.7986 - val_loss: 0.9633 - val_accuracy: 0.8114\n",
            "Epoch 37/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9947 - accuracy: 0.7973 - val_loss: 0.9665 - val_accuracy: 0.8126\n",
            "Epoch 38/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9937 - accuracy: 0.7978 - val_loss: 0.9754 - val_accuracy: 0.8164\n",
            "Epoch 39/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9898 - accuracy: 0.8006 - val_loss: 1.0454 - val_accuracy: 0.7974\n",
            "Epoch 40/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9893 - accuracy: 0.8014 - val_loss: 1.0886 - val_accuracy: 0.7734\n",
            "Epoch 41/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9866 - accuracy: 0.8010 - val_loss: 0.9343 - val_accuracy: 0.8248\n",
            "Epoch 42/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9890 - accuracy: 0.7998 - val_loss: 0.9570 - val_accuracy: 0.8126\n",
            "Epoch 43/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9775 - accuracy: 0.8023 - val_loss: 0.9294 - val_accuracy: 0.8215\n",
            "Epoch 44/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9806 - accuracy: 0.8027 - val_loss: 0.8904 - val_accuracy: 0.8341\n",
            "Epoch 45/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9798 - accuracy: 0.8030 - val_loss: 0.9285 - val_accuracy: 0.8274\n",
            "Epoch 46/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9800 - accuracy: 0.8035 - val_loss: 0.9533 - val_accuracy: 0.8197\n",
            "Epoch 47/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9777 - accuracy: 0.8047 - val_loss: 0.9413 - val_accuracy: 0.8202\n",
            "Epoch 48/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9791 - accuracy: 0.8028 - val_loss: 0.9213 - val_accuracy: 0.8313\n",
            "Epoch 49/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9764 - accuracy: 0.8047 - val_loss: 0.9517 - val_accuracy: 0.8126\n",
            "Epoch 50/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9726 - accuracy: 0.8061 - val_loss: 0.9945 - val_accuracy: 0.8033\n",
            "Epoch 51/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9748 - accuracy: 0.8054 - val_loss: 0.9988 - val_accuracy: 0.8054\n",
            "Epoch 52/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9687 - accuracy: 0.8068 - val_loss: 0.9641 - val_accuracy: 0.8109\n",
            "Epoch 53/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9685 - accuracy: 0.8057 - val_loss: 0.9235 - val_accuracy: 0.8287\n",
            "Epoch 54/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9669 - accuracy: 0.8069 - val_loss: 0.8968 - val_accuracy: 0.8322\n",
            "Epoch 55/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9706 - accuracy: 0.8062 - val_loss: 0.9512 - val_accuracy: 0.8175\n",
            "Epoch 56/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9608 - accuracy: 0.8075 - val_loss: 0.9696 - val_accuracy: 0.8157\n",
            "Epoch 57/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9682 - accuracy: 0.8060 - val_loss: 0.9769 - val_accuracy: 0.8082\n",
            "Epoch 58/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9671 - accuracy: 0.8037 - val_loss: 0.8774 - val_accuracy: 0.8417\n",
            "Epoch 59/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9600 - accuracy: 0.8079 - val_loss: 1.0079 - val_accuracy: 0.7992\n",
            "Epoch 60/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9584 - accuracy: 0.8071 - val_loss: 0.9558 - val_accuracy: 0.8107\n",
            "Epoch 61/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9547 - accuracy: 0.8081 - val_loss: 0.8766 - val_accuracy: 0.8401\n",
            "Epoch 62/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9510 - accuracy: 0.8105 - val_loss: 0.8634 - val_accuracy: 0.8441\n",
            "Epoch 63/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9552 - accuracy: 0.8082 - val_loss: 0.9258 - val_accuracy: 0.8196\n",
            "Epoch 64/75\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.9536 - accuracy: 0.8087 - val_loss: 0.9398 - val_accuracy: 0.8175\n",
            "Epoch 65/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9551 - accuracy: 0.8072 - val_loss: 0.9180 - val_accuracy: 0.8298\n",
            "Epoch 66/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9537 - accuracy: 0.8078 - val_loss: 0.9629 - val_accuracy: 0.8154\n",
            "Epoch 67/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9532 - accuracy: 0.8090 - val_loss: 0.8888 - val_accuracy: 0.8349\n",
            "Epoch 68/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9471 - accuracy: 0.8096 - val_loss: 0.9163 - val_accuracy: 0.8250\n",
            "Epoch 69/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9476 - accuracy: 0.8083 - val_loss: 0.9704 - val_accuracy: 0.8056\n",
            "Epoch 70/75\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.9522 - accuracy: 0.8076 - val_loss: 0.9309 - val_accuracy: 0.8172\n",
            "Epoch 71/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9472 - accuracy: 0.8100 - val_loss: 0.9301 - val_accuracy: 0.8205\n",
            "Epoch 72/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9476 - accuracy: 0.8108 - val_loss: 0.9586 - val_accuracy: 0.8132\n",
            "Epoch 73/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9465 - accuracy: 0.8095 - val_loss: 0.9312 - val_accuracy: 0.8222\n",
            "Epoch 74/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9478 - accuracy: 0.8097 - val_loss: 0.9345 - val_accuracy: 0.8215\n",
            "Epoch 75/75\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9462 - accuracy: 0.8104 - val_loss: 0.9268 - val_accuracy: 0.8215\n",
            "Epoch 1/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.8739 - accuracy: 0.8279 - val_loss: 0.8501 - val_accuracy: 0.8334\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8445 - accuracy: 0.8311 - val_loss: 0.8013 - val_accuracy: 0.8415\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.8209 - accuracy: 0.8344 - val_loss: 0.7952 - val_accuracy: 0.8453\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.8217 - accuracy: 0.8331 - val_loss: 0.8338 - val_accuracy: 0.8351\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8061 - accuracy: 0.8365 - val_loss: 0.8250 - val_accuracy: 0.8373\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8117 - accuracy: 0.8330 - val_loss: 0.7995 - val_accuracy: 0.8394\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8061 - accuracy: 0.8331 - val_loss: 0.7800 - val_accuracy: 0.8432\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8048 - accuracy: 0.8332 - val_loss: 0.8320 - val_accuracy: 0.8301\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8024 - accuracy: 0.8332 - val_loss: 0.8035 - val_accuracy: 0.8350\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7994 - accuracy: 0.8329 - val_loss: 0.7627 - val_accuracy: 0.8481\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8024 - accuracy: 0.8315 - val_loss: 0.8411 - val_accuracy: 0.8228\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7976 - accuracy: 0.8338 - val_loss: 0.7828 - val_accuracy: 0.8402\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7926 - accuracy: 0.8351 - val_loss: 0.7732 - val_accuracy: 0.8446\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7900 - accuracy: 0.8346 - val_loss: 0.9254 - val_accuracy: 0.8037\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.7903 - accuracy: 0.8339 - val_loss: 0.7959 - val_accuracy: 0.8358\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7869 - accuracy: 0.8352 - val_loss: 0.7656 - val_accuracy: 0.8452\n",
            "Epoch 17/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7876 - accuracy: 0.8346 - val_loss: 0.8104 - val_accuracy: 0.8298\n",
            "Epoch 18/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7911 - accuracy: 0.8329 - val_loss: 0.7308 - val_accuracy: 0.8550\n",
            "Epoch 19/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7887 - accuracy: 0.8330 - val_loss: 0.7362 - val_accuracy: 0.8496\n",
            "Epoch 20/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7886 - accuracy: 0.8335 - val_loss: 0.8027 - val_accuracy: 0.8324\n",
            "Epoch 21/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7884 - accuracy: 0.8334 - val_loss: 0.7455 - val_accuracy: 0.8493\n",
            "Epoch 22/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7902 - accuracy: 0.8336 - val_loss: 0.7835 - val_accuracy: 0.8385\n",
            "Epoch 23/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7847 - accuracy: 0.8356 - val_loss: 0.8451 - val_accuracy: 0.8205\n",
            "Epoch 24/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7867 - accuracy: 0.8337 - val_loss: 0.7757 - val_accuracy: 0.8405\n",
            "Epoch 25/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7825 - accuracy: 0.8346 - val_loss: 0.7536 - val_accuracy: 0.8499\n",
            "Epoch 1/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7489 - accuracy: 0.8442 - val_loss: 0.7245 - val_accuracy: 0.8526\n",
            "Epoch 2/25\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.7297 - accuracy: 0.8471 - val_loss: 0.7226 - val_accuracy: 0.8479\n",
            "Epoch 3/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7234 - accuracy: 0.8481 - val_loss: 0.7241 - val_accuracy: 0.8508\n",
            "Epoch 4/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7185 - accuracy: 0.8487 - val_loss: 0.7032 - val_accuracy: 0.8517\n",
            "Epoch 5/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7173 - accuracy: 0.8488 - val_loss: 0.6857 - val_accuracy: 0.8555\n",
            "Epoch 6/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7104 - accuracy: 0.8497 - val_loss: 0.7348 - val_accuracy: 0.8420\n",
            "Epoch 7/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7116 - accuracy: 0.8469 - val_loss: 0.7324 - val_accuracy: 0.8442\n",
            "Epoch 8/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7029 - accuracy: 0.8488 - val_loss: 0.7160 - val_accuracy: 0.8482\n",
            "Epoch 9/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7077 - accuracy: 0.8489 - val_loss: 0.7584 - val_accuracy: 0.8346\n",
            "Epoch 10/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7099 - accuracy: 0.8464 - val_loss: 0.7402 - val_accuracy: 0.8400\n",
            "Epoch 11/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7002 - accuracy: 0.8493 - val_loss: 0.6487 - val_accuracy: 0.8687\n",
            "Epoch 12/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6987 - accuracy: 0.8488 - val_loss: 0.7073 - val_accuracy: 0.8484\n",
            "Epoch 13/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6997 - accuracy: 0.8508 - val_loss: 0.6845 - val_accuracy: 0.8567\n",
            "Epoch 14/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6976 - accuracy: 0.8499 - val_loss: 0.6924 - val_accuracy: 0.8576\n",
            "Epoch 15/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.7014 - accuracy: 0.8489 - val_loss: 0.7109 - val_accuracy: 0.8463\n",
            "Epoch 16/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6939 - accuracy: 0.8513 - val_loss: 0.7714 - val_accuracy: 0.8320\n",
            "Epoch 17/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6899 - accuracy: 0.8508 - val_loss: 0.7047 - val_accuracy: 0.8494\n",
            "Epoch 18/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6937 - accuracy: 0.8497 - val_loss: 0.6671 - val_accuracy: 0.8600\n",
            "Epoch 19/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6932 - accuracy: 0.8522 - val_loss: 0.6969 - val_accuracy: 0.8516\n",
            "Epoch 20/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6927 - accuracy: 0.8486 - val_loss: 0.6801 - val_accuracy: 0.8549\n",
            "Epoch 21/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6908 - accuracy: 0.8507 - val_loss: 0.7111 - val_accuracy: 0.8505\n",
            "Epoch 22/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6842 - accuracy: 0.8529 - val_loss: 0.6568 - val_accuracy: 0.8657\n",
            "Epoch 23/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6904 - accuracy: 0.8508 - val_loss: 0.7294 - val_accuracy: 0.8416\n",
            "Epoch 24/25\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6862 - accuracy: 0.8508 - val_loss: 0.7400 - val_accuracy: 0.8408\n",
            "Epoch 25/25\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.6869 - accuracy: 0.8506 - val_loss: 0.6817 - val_accuracy: 0.8565\n",
            "79/79 [==============================] - 0s 5ms/step - loss: 0.6817 - accuracy: 0.8565\n",
            "\n",
            "Test result: 85.650 loss: 0.682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9NML26fYNc8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}